{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Lambda studies","text":"Version <p>Created 01/2024. Update 02/2024</p> <p>This repository includes a set of best practices around developing a solution using Lambda and Amazon serverless services. To demonstrate some of the concepts with code, the repository also includes a simple microservice to manage an inventory of Autonomous Robot Cars.</p>"},{"location":"#questions-this-repository-try-to-answer","title":"Questions this repository try to answer","text":""},{"location":"#architecture-patterns","title":"Architecture Patterns","text":"<ul> <li> Review Lambda runtime architecture (internal view). See this section &gt;&gt;&gt; </li> <li> Serverless architecture patterns: how to map to a microservice approach ? what are the best practices for using Lambda functions to ensure loose coupling and service autonomy? </li> <li> How to use event sourcing with Lambda? </li> <li> How to handle state management in a stateless environment like AWS Lambda, especially for complex workflows or transactions?</li> <li> What are the best practices for integrating AWS Lambda with other AWS services, particularly for event-driven architectures?</li> <li> How to design Lambda functions to handle events from multiple sources, and what considerations are there for managing event source mappings?</li> <li> How do AWS Lambda's service limits impact architectural decisions, and what strategies can be used to mitigate potential limitations? Answer&gt;&gt;&gt;</li> <li> Are there specific scenarios where AWS Lambda might not be the best choice, and what alternatives to consider?</li> <li> How to integrate API Gateway with Lambda functions to scale demand? </li> <li> How to use AWS Step Functions to orchestrate multiple Lambda functions for complex workflows?</li> <li> What are the best practices for using event source mappings with Lambda to process records from streams or queues?</li> </ul> <p>&gt;&gt;&gt; See discussions in architecture section</p>"},{"location":"#scaling","title":"Scaling","text":"<ul> <li> How to scale lambda for high-traffic applications and how to monitor scaling?</li> <li> How to optimize Lambda function for performance?</li> <li> Review strategy to minimize cold start?</li> <li> Optimizing for high-throughput?</li> <li> How does the allocated memory size affect the execution time and cost of Lambda functions?</li> <li> How to balance performance / cost?</li> </ul> <p>&gt;&gt;&gt; See discussions in scaling section</p>"},{"location":"#security","title":"Security","text":"<ul> <li> What are the best practices to secure Lambda functions, especially in terms of managing permissions and access controls? [A &gt;&gt;&gt;](./security.md</li> <li> How should sensitive data be managed within Lambda environments?</li> <li> How to manage secrets? A &gt;&gt;&gt;</li> <li> How to manage IAM roles and permissions for Lambda functions to ensure principle of least privilege?</li> <li> How to support encrypting sensitive data processed by Lambda functions, both in transit and at rest?</li> </ul> <p>&gt;&gt;&gt; See discussions in security section</p>"},{"location":"#cost-optimization","title":"Cost Optimization","text":"<ul> <li> How to optimize AWS Lambda costs for a large-scale application, and what tools or metrics are most useful for monitoring and controlling Lambda expenses?</li> <li> Are there specific patterns or architectural choices that significantly affect cost, and what are the considerations to balance cost with performance?</li> <li> How does AWS Lambda's pricing model scale beyond the free tier, and what measures can be taken to predict and control expenditure?</li> </ul> <p>&gt;&gt;&gt; See discussions in cost section</p>"},{"location":"#development-operation-and-cicd","title":"Development, Operation and CI/CD","text":"<ul> <li> What are the best practices for deploying Lambda functions in a CI/CD pipeline, and how to manage version control and rollbacks?</li> <li> How to support blue-green deployments or canary releases with AWS Lambda? A &gt;&gt;&gt;</li> <li> How should error handling be implemented in AWS Lambda to ensure reliability and fault tolerance?</li> <li> What to monitor and logging tools or practices do you recommend for AWS Lambda functions to ensure proactive issue resolution?</li> <li> What tools to do local development and testing of AWS Lambda functions?</li> <li> How can teams effectively debug Lambda functions, particularly when integrated with other AWS services?</li> <li> How to adopt CI/CD practices for Lambda functions? How to automate deployment while ensuring rollback capabilities for stability?</li> <li> How to effectively use versioning and aliases in AWS Lambda to manage deployments and facilitate A/B testing?</li> <li> How to implement anomaly detection in Lambda executions to quickly identify and respond to unusual patterns or errors?</li> </ul> <p>&gt;&gt;&gt; See discussions in development and operations section</p>"},{"location":"#a-demonstration-application","title":"A demonstration application","text":"<p>To support the discussions addressed in this repository, I will start by a simple application using API Gateway, Lambda, DynamoDB and add some logging and monitoring features on top of it.</p> <p>The application starts with the simplest deployment as illustrated in the following diagrams:</p> <p></p> <p>To be more event-driven microservice, I add event-bridge as middleware to produce messages to.</p> <p></p>"},{"location":"#features-to-consider","title":"Features to consider","text":"<ul> <li> Use PowerTools APIGatewayResolver to use same lambda for CRU on car entity. With unit tests.</li> <li> Deploy the app with CDK and API Gateway lambda proxy integration. </li> <li> Add SQS after API Gateway and get Lambda pulling the messages. Add DLQ</li> </ul>"},{"location":"architecture/","title":"Architecture considerations","text":"<p>AWS Lambda helps you to run code without managing server. But this does not mean it is the unique serverless capability in AWS, and you should consider adopting good design principles that we can summarize as:</p>"},{"location":"architecture/#design-principles-for-cloud-deployment","title":"Design principles for cloud deployment","text":"<ul> <li>Stop guessing your capacity needs: use as much or as little capacity as you need, and scale up and down automatically.</li> <li>Test systems at production scale, then decommission the development resources.</li> <li>Automate to make architectural experimentation easier, using infrastructure as code.</li> <li>Allow for evolutionary architectures: the capability to automate and test on demand, lowers the risk of impact from design changes. This allows systems to evolve over time so that businesses can take advantage of innovations as a standard practice.</li> <li>Drive architectures using data: In the cloud, you can collect data on how your architectural choices affect the behavior of your workload. This lets you make fact-based decisions on how to improve your workload.</li> <li>Improve through game days to simulate events in production. This will help you understand where improvements can be made and can help develop organizational experience in dealing with events.</li> </ul>"},{"location":"architecture/#microservice-and-aws-lambda","title":"Microservice and AWS Lambda","text":"<p>The Lambda code fits in the function programming paradigm. As a twelve factors app Lambda function has the characteristic of a microservice. The entry point is a unique handler function, which constraint the implementation or enforce implementing message routing. The payload is call event, but is a loose definition as agreed upon in classical event-driven architecture, where it represents something happen as immutable fact. In microservice design, developer has to think about the domain-driven-design (DDD) bounded context. The methodology starts with the event storming practice and applies DDD constructs like Command, Aggregate, Event, Repository... Those elements are the grouping that serve as foundations for the microservice implementation. The following figure illustrates how to move from elements discovered during DDD sessions and a microservice supports the Order Entity:</p> <p></p> <p>Figure 1: Event-driven microservice design elements</p> <ul> <li>The Commands are mapped to APIs</li> <li>The Entity and Value Objects will be persisted via a Repository</li> <li>Events are defined with schema and produce/consume via a messaging API layer</li> </ul> <p>The common practice to develop microservice is to adopt a test-driven development, which leads to define and tune the API definition bottom-up. The same apply for the business logic, and the event definitions. Which means the API definition and contract are built from code and shared as an external definition using the OpenAPI standard. The same applies for the asynchronous communication of the events. For that the standard is  asyncAPI document can be generated from code. In Java world, Quarkus and microprofile facilitate the implementation of microservice. Nodejs has the expressjs library to define API. Code organization helps to support a single unit deployment. For Python, Flask is a common library to expose REST api.</p> <p>Those practices lead to have one Git Repository per microservice, with infrastructure as code to build a container and deploy to target runtime (Kubernetes cluster, AWS ECS Fargate, AWS Lambda). A microservice development ownership is limited to few persons. Microservices do not need to be event-driven on day one. It is important to know and understand the microservice challenges and the scalability requirements of the service. When the number of service consumers and the number of requests grow above a certain level, and the service level operations are no more well serve, then refactoring are needed to move to the asynchronous, and horizontal scaling (See why EDA is an evolution of SOA).</p>"},{"location":"architecture/#lambda-to-implement-microservice","title":"Lambda to implement microservice","text":"<p>While Lambda seems to map to microservice principles of separating an application into distinct units of independent deployment, it is still a function programming. There is nothing wrong about that, but the design and granularity of the business logic to fit into this function handler may violate some other software engineering principles, like encapsulation, clear separation of concern, avoid unnecessary complexity, code duplication. Taking the same DDD elements for the Order microservice, we can try to map it into Lambdas and other services to support the same bounded context:</p> <p></p> <p>Figure 2: From DDD elements to Lambda as microservice</p> <p>The command definitions are done in Amazon API Gateway elements not in the form of OpenAPI. To support the DDD Commands implementation in Lambda there are two choices: 1/ one Lambda per REST resource verbs, or 2/ group in one function handler with routing depending of the HTTP method used. </p> <p>With the first implementation choice, the microservice is now a group of lambda functions, API gateway construct, Dynamodb tables (or SQL ones, does not matter for this discussion), and queueing for event propagation. The code integrity is done via the Infrastructure as Code (IaC) definition. This IaC can be done with AWS CloudFormation template, CDK or AWS Serverless Application Model SAM.</p> <p>The AWS Powertool REST API supports the second pattern with annotation. The first solution means duplicate code, and complex infrastructure as code, while the second is more 'microservice' principle oriented, simpler to unit test and to package. When not using AWS Powertool RestOrchestration, the routing logic is boilerplate code with minimum business logic. </p> <pre><code>def handler(command, context):\n    httpMethod = command['httpMethod']\n    if httpMethod == 'GET':\n        car_id = command['pathParameters']['car_id']\n        car = getCar(car_id)\n        return car\n    elif httpMethod == 'POST':\n        carStr=command['body']\n        car=json.loads(carStr)\n        carOut = createCar(car)\n</code></pre> <p>While with RestResolver the boilerplate code is abstracted via annotations:</p> <pre><code>@app.get(\"/cars/&lt;car_id&gt;\")\ndef getCarUsingCarId(car_id: str):\n\ndef handler(message: dict, context: LambdaContext) -&gt; dict:\n    return app.resolve(message, context)\n</code></pre> <p>Also this is not because the argument of the function is called 'event' that it has the same semantic of a business event in an EDA. This is a message to process as a request, and with API Gateway frontend it is a HTTP request.</p> <p>When adopting and event-driven implementation, we need to assess if we want to use the Transactional Outbox pattern, by saving the event to a dedicate event table, and use Change Data Capture tool or DynamoDB streaming capability to propagate events to the downstream consumers.</p>"},{"location":"architecture/#lambda-constructs","title":"Lambda constructs","text":"<p>A  Lambda function has three primary components \u2013 trigger, code, and configuration.</p> <p></p> <p>Figure 3: Lambda constructs</p> <ul> <li>Triggers describe when a Lambda function should run. A trigger integrates the Lambda function with other AWS services, enabling to run the Lambda function in response to certain API calls that occur in the AWS account. To support microservice implementation using Lambda, the main trigger should be API Gateway.</li> <li>An execution environment manages the processes and resources that are required to run the function. </li> <li>Configuration includes compute resources, execution timeout, IAM roles (lambda_basic_execution)...</li> </ul> <p>Servers do not run continuously, and a function invocation is time boxed to 15 minutes. </p>"},{"location":"architecture/#lambda-run-time-architecture","title":"Lambda run time architecture","text":"<ul> <li> <p>The Lambda service is split into the control plane and the data plane. The control plane provides the management APIs (for example, <code>CreateFunction</code>, <code>UpdateFunctionCode</code>). The data plane is where Lambda's API resides to invoke the Lambda functions. It is HA over multi AZs in same region.</p> <p></p> <p>Figure 4: Control and Data Planes</p> </li> <li> <p>Lambda Workers are bare metal Amazon EC2 Nitro instances which are launched and managed by Lambda in a separate isolated AWS account which is not visible to customers. Each worker has one to many Firecraker microVMs. There is no container engine. Container image is just for packaging the lambda code as zip does. The following diagram presents the high level components, that are slightly different if the communication with the client is synchronous or asynchronous:</p> <p></p> <p>Figure 5: Lambda services high level component view</p> <p>The event source mapping is a Lambda components that reads from an event source and invokes a Lambda function (see details below).</p> </li> <li> <p>The execution environment follows the life cycle as defined below (time goes from left to right):</p> <p></p> <p>Figure 6: Lambda execution environment lifecycle</p> </li> <li> <p>In the Init phase, Lambda creates or unfreezes an execution environment with the configured resources, downloads the code for the function with all the needed layers, initializes any extensions, initializes the runtime, and then runs the function\u2019s initialization code. After init, the environment is 'Warm'. The extension and runtime initializations are part of the <code>cold start</code> (&lt;1s).</p> </li> <li>In the Invoke phase, Lambda invokes the function handler. After the function runs to completion, Lambda prepares to handle another function invocation. After the execution completes, the execution environment is \"frozen\", for reuse by another request. (Warm start)</li> <li>During Shutdown phase: Lambda shuts down the runtime, alerts the extensions to let them stop cleanly, and then removes the environment.</li> </ul> Lambda Extension <p>Lambda supports external and internal extensions. An external extension runs as an independent process in the execution environment and continues to run after the function invocation is fully processed. Can be used for logging, monitoring, integration...</p> <ul> <li>A function's concurrency is the number of execution environment instances that serve requests at a given time.</li> <li> <p>There is at least one runtime which matches the programming language (Java, Node.js, C#, Go, or Python).</p> </li> <li> <p>Synchronous calls are used for immediate function response, with potential errors returned to the caller. It may return throttles when the number of requests hits the concurrency limit.</p> </li> </ul> A synchronous flow to Lambda explained <p>Below is a diagram representing an external client application invoking a Lambda function synchronously. The requests reach one of the front end load balancer, which routes the request to a Frontend lambda invoke service. As a multi-AZ service, for high availability, there is no concern of load balancing across AZs. </p> <p></p> <p>Figure 7: Synchronous flow to lambda function</p> <p>The frontend invoke svc performs authentication and authorization of the request to ensure only authorized apps can access the function. It also calls the Counting svc, to assess quota limits based on reserver concurrency, burst, counts... The Assignment svc, is used as stateful service, to find a worker to assign to the Lambda function (it also defines the CPU, memory needed by the fct). The control plane svc manages the fct creation, the life cycle of the assigned service nodes. The Assignment svc is a coordinator, information retriever, and distributor about what state execution environments are on the worker host and where invokes should go. It keeps frontend svc up to date about fct execution. For the first invocation, it also communicates with the Placement svc to get new runtime environment to be started on a worker host with a time-based lease. It uses Optimization to maximize packing density for better fleet utilization and minimize cold start latency. Placement svc monitors worker health. Once the environment is up and running, the Assignment svc gets the supplied IAM role for the lambda function security privileges defined and distributed to the worker host, along with the environment variables. Once done the Frontend svc can do the invoke of the lambda fct handler. When the response is sent back to the client, the worker signals back to the Assignment svc the invoke is done, so it can receive new warm invocations.</p> <ul> <li>Asynchronous call goes to an \"event invoke\" frontend, that returns an acknowledgement message. Event payloads are always queued for processing before invocation. Asynchronous processing should be more scalable.</li> </ul> Asynchronous flow to Lambda details <p>The Load balancers knows it is an asynchronous or event invoke so it routed to an \"event invoke\" frontend svc. The request message or event is enqueued to SQS queue. There are multiple autoscaling SQS queues, and they are hidden to the lambda developer.</p> <p> </p> <p>Figure 8: Asynchronous flow to lambda function</p> <p>Internal SQS queue persists messages for up to 6 hours. Queued events are retrieved in batches by Lambda\u2019s poller instances, which also delete messages from the queue. The poller fleet is a group of Amazon EC2 instances whose purpose is to process queued event invocations which have not yet been processed. The poller propagates the invoke using the synchronous \"frontend invoke svc\". Once the invoke to the handler succeed, the poller gets the response and can delete the message from the queue. When the message fails all processing attempts, it will follow the same visibility timeout as in SQS, the message is discarded by Lambda. The dead letter queue (DLQ) feature allows sending unprocessed events from asynchronous invocations to an Amazon SQS queue or an Amazon SNS topic defined by the developer. The Queue manager svc manages queues, monitors them for any backups. It interacts with the Leasing svc which manages which pollers are processing with which queues. The Leasing svc also manages and monitors pollers.</p> <p>???+ info Event source mapping     Event source mapping is used to poll messages from different streaming sources and then synchronously calls the Lambda function. It reads using batching and sends all the events as argument to the function. If the function returns an error for any of the messages in a batch, Lambda retries the whole batch of messages until processing succeeds or the messages expire. It supports error handling. It is based on the same architecture as the asynchronous invoke frontend and poller fleet, except the code of the poller is specific to the event source. </p> <pre><code>![](./diagrams/evt-mapping-lambda-flow.drawio.png){width=800}\n\n**Figure 9: Event source processing to synch lambda**\n\nAs a managed service within Lambda the poller fleet is serverless too.\n</code></pre> <ul> <li>If the Lambda service is not available. Callers may queue their payload on the client-side for retry.</li> </ul>"},{"location":"architecture/#event-sourcing","title":"Event sourcing","text":"<p>Event sourcing is a design pattern often used in EDA adoption. Most business applications are state based persistent where any update changes the previous state of the business entities. The database keeps the last committed update. But some business application needs to explain how an entity reaches its current state. For that, the application needs to keep history of business facts.</p> <p>Event sourcing persists the state of a business entity, such an Order, as a sequence of state-changing events or immutable \"facts\", ordered over time. Event sourcing has its roots in the domain-driven design community.</p> <p>When the state of a system changes, an application issues a notification event of the state change. </p> <p></p> <p>Figure 10: event sourcing over time</p> <p>Any interested parties can become consumers of the event and take required actions.  The state-change event is immutable stored in an event log or event store in time order.  The event log becomes the principal source of truth. The system state can be recreated from a point in time by reprocessing the events. The history of state changes becomes an audit record for the business and is often a useful source of data for business analysts to gain insights into the business.</p> <p>Therefore Lambda function, when it manages a business entity, can generate events to a event backbone, or persists the event in a time based data store like Amazon Timestream, which supports trillions of data points per day, keeping recent data in memory and moving historical data to a cost optimized storage tier based upon user defined policies. It includes a query engine with time based analytic functions.</p> <p></p> <p>Figure 11: Outbox pattern with Amazon Timestream</p> <p>Timestream is serverless and automatically scales up or down to adjust capacity and performance. The architecture uses different layers to increase decoupling, and a cellular architecture using smaller copies of the time tables, with specific networking end-points.</p> <p>An alternate to Lambda directly writing to TimeStream, the Lambda can write in an event outbox table, and the Change Data Capture or a Streaming feature can push the event to a queue then to TimeStream.</p> <p></p> <p>Figure 12: Outbox pattern with Dynamodb and DynamoDB streams</p> <p>Some implementation leverages Kafka to support event sourcing. The major question will be how long in the past the application needs to come back to. Kafka has a retention period, that can be tuned at the topic level, so if the retention is not covering the expected playback period, then event sourcing will miss events.</p>"},{"location":"architecture/#fit-for-purpose","title":"Fit for purpose","text":"<p>Before starting big implementation it is important to verify which technology to select. A lot of projects fail by selecting the wrong software to support the business application (\"With an hammer everything is a nail\" paradigm). For Lambda there are technology and application design constraints that need to be reviewed:</p>"},{"location":"architecture/#technical-constraints","title":"Technical constraints","text":"<p>There are a set of physical constraints the Lambda application needs to support:</p> <ul> <li>Must run under 15 min.</li> <li>Memory from 128MB to 10GB.</li> <li>Maximum 1000 concurrent calls.</li> <li>Code in compressed zip should be under 50MB and 250MB uncompressed.</li> <li>Latency on the first call.</li> <li>Disk capacity for /tmp is limited to 10GB.</li> </ul>"},{"location":"architecture/#design-constraints","title":"Design constraints","text":"<ul> <li>Event-driven architectures use events to communicate between services and invoke decoupled services. </li> <li>When migrating existing application review the different design patterns to consider.</li> <li>Think about co-existence with existing application and how API Gateway can be integrated to direct traffic to new components (Lambda functions) without disrupting existing systems. With API Gateway, developer may export the SDK for the business APIs to make integration easier, and can use throttling and usage plans to control how different clients can use the API.</li> <li>Do cost comparison analysis. For example API Gateway is pay by the requests, while ALB is priced by hours based on the load balance capacity units used per hour. Lambda is also pay per request based.</li> <li>Assess when to use ECS Fargate when the application is in container, and may run for a long time period. Larger packaging may not be possible to run on Lambda, while may run easily on ECS. Applications that use non HTTP end point, integrate to messaging middleware with Java based APIs (Kafka, AMQP,...) are better fit for ECS Fargate deployment.</li> </ul>"},{"location":"architecture/#designing-for-lambda","title":"Designing for Lambda","text":"<p>A lot of design principles are well documented in the Lambda user guide, below are some complement principles to consider.</p>"},{"location":"architecture/#migrating-existing-workloads","title":"Migrating existing workloads","text":"<p>Some of the common design patterns to consider when migrating to serverless:</p> <ul> <li>Leapfrog: bypass interim steps and go straight from an on-premises legacy architecture to a serverless cloud architecture.</li> <li>Organic: lift and shift. Experiment with Lambda in low-risk internal scenarios such as log processing or cron jobs.</li> <li>Strangler: incrementally and systematically decomposes monolithic applications by creating APIs and building event-driven components that gradually replace components of the legacy application. New feature branches can be serverless first, and legacy components can be decommissioned as they are replaced.</li> </ul>"},{"location":"architecture/#good-questions-to-assess","title":"Good questions to assess","text":"Question Comment What does the application do and how are its components organized? Establish a bounded context for each microservice. Assess entity and data store. How can you break your data needs based on the command query responsibility segregation (CQRS) pattern? Strangle data by needs. Decouple write transaction from queries at scale. How does the application scale and what components drive the capacity you need? Look at independent scaling, and high scaling demand. Do you have schedule-based tasks? cron jobs are good targets to replace with Lambda functions Do you have workers listening to a queue? Decoupling may already being adopted, EDA discussions may be started. Where can you refactor or enhance functionality without impacting the current implementation? Look at API needs, ALB needs."},{"location":"architecture/#starting-new-application","title":"Starting new application","text":"<p>When starting development for a new application, the following principles may be reviewed to assess for their pertinence:</p> <ul> <li>Design each Lambda function to handle a single, focused task without dependencies on other functions. This keeps functions independent and decoupled.</li> <li>Apply the DDD and bounded context principles to identify service granularity.</li> <li>Assess the need for synchronous call between function, and when orchestration flow needs to be externalized. Developer may reduce coupling by adopting asynchronous invocation between functions but it adds complexity and cost.</li> <li>Verify is the performance needs are on the read and query part, and adopt CQRS.</li> <li>When adopting asynchronous invocation, think of message duplications and how to support idempotency.</li> <li>Implement error handling within each function rather than propagating errors up the call chain. This prevents failures in one function from impacting others.</li> </ul> <p>See Serverless app Lens in AWS Well Architected.</p>"},{"location":"architecture/#lambda-and-event-driven-architecture","title":"Lambda and Event-driven architecture","text":"<p>Event-driven architectures use events to communicate between services and invoke decoupled services.</p> <p>The reference architecture for Event-driven architecture is described in this article with a focus to the 'event backbone' expected capabilities. To simplify, I will claim, a event backbone includes a messaging middleware with persistence, schema management, supporting point to point queue, pub-sub topics, and streaming capabilities.</p> <p>One important consideration is to address what are the event producers, the type of events and what consumers are interested by those events.</p> <p>Lambda Function can be producer of business events to an event backbone.</p> <p>It can also be a consumer, and Lambda has native support for events produced by message and streaming services like Amazon Simple Queue Service (SQS), Amazon Simple Notification Service (SNS), and Amazon Kinesis. The major question is at once delivery semantic and idempotency support to help consumers processing duplicate.</p>"},{"location":"architecture/#integration-with-other-services","title":"Integration with other services","text":"<p>AWS Lambda has a lot of other Amazon service integrations. We will focus here on the most commons:</p>"},{"location":"architecture/#apis","title":"APIs","text":"<p>API Gateway can define webSocket, HTTP or REST APIs integrated with Lambda function. HTTP APIs are newer and are built with the API Gateway version 2 API. REST APIs support more options for authentication and authorization.. In most case one HTTP verb and path are mapped to one handler function, but it should not be the best approach: some lambdas are implementing a microservice to manage a business entity and may support the RESTful operations on the entity. The resource is <code>cars</code> and HTTP GET, POST, PUT, DELETE may be mapped to one function. In this case the full HTTP request is passed to lambda. AWS Powertools can be used to route to the good function to process the request.</p> <pre><code>app = APIGatewayRestResolver()\n\n@app.get(\"/cars/&lt;car_id&gt;\")\n@tracer.capture_method\ndef getCarUsingCarId(car_id: str):\n    return car_repository.getCarUsingCarId(car_id=car_id)\n\n\ndef handler(message: dict, context: LambdaContext) -&gt; dict:\n    return app.resolve(message, context)\n</code></pre> <p>Amazon API Gateway gets permission to invoke the function from the function's resource-based policy with the principal being the <code>apigateway.amazonaws.com</code>. </p> <p>See APIGateway getting started tutorial and API Gateway workshop.</p>"},{"location":"architecture/#function-urls","title":"Function URLs","text":"<p>Lambda also supports function URLs, a built-in HTTPS endpoint for invoking functions. No need for API Gateway and ALB.</p>"},{"location":"architecture/#eventbridge-as-a-source","title":"EventBridge as a source","text":"<p>EventBridge are a classical source of events for Lambda processing as itself integrate with a lot of AWS services. The most important design consideration is to review the event structure generated by EventBridge, review which event bus is used: the <code>default</code> for AWS service events, or custom event bus for any custom applications.</p>"},{"location":"architecture/#multi-event-sources","title":"Multi event sources","text":""},{"location":"architecture/#queueing-integration","title":"Queueing integration","text":""},{"location":"architecture/#streaming-integration","title":"Streaming integration","text":""},{"location":"architecture/#service-limits-or-quotas","title":"Service limits or quotas","text":"<p>By design Lambda service scale the function invocation to serve the traffic of the application. Other services integrated with Lambda, like API Gateway, SNS, SQS and Step Functions, also scale up to respond to increased load. </p> <p>As a multi-tenant service, there are guardrails in place. Service quotas can be raised until a hard limit.</p> <p>Quotas may apply at the Region level, or account level, and may also include time-interval restrictions.</p> <p>See the designing with service quotas section of the product user guide.</p> <p>Pay attention to concurrent requests quotas per service, and payload size constraint. With big payload think about the claim-check pattern where big files are saved to external storage like S3 bucket.</p> <p>As some quotas are set a the account level. First separate dev and production accounts. Leverage AWS Organizations to manage all those accounts with the IAM security policies... Use developer account to be able to test lambda with some production data, without impacting the quota limits... Add more accounts over time to address specific deployment, growth per region.</p> <p>See the scaling section...</p>"},{"location":"cost/","title":"Cost optimization","text":"<p>The Lambda execution is payed by the millisecond. The billing metrics used is the duration as equal to allocated memory in GB x execution time in seconds.</p> <p>The cost = duration x number of invocations.</p>"},{"location":"dev_ops/","title":"DevOps","text":"<p>This section addresses development practices and operations for Lambda application. </p> <p>As solution deployed on AWS, and integrating with AWS services, developers need to study the SDK (e.g. Python boto3 module).</p> <p>There are a lot of source of information, starting by aws.amazon.com/devops/.</p>"},{"location":"dev_ops/#development-tooling","title":"Development tooling","text":"<p>There is no need to be prescriptive in this domain as each developer has his/he own habit. However as the solution will run on AWS cloud there are a lot of nice tools and open source content developers should consider.</p> <p>The core element of the infrastructure as code (IaC) on AWS is AWS CloudFormation. If developers or DevOps engineers prefer to use programming language to define service configuration and code deployment the AWS Cloud Development Kit is a powerful solution. It uses code which can be integrated in the same Git repository as application / microservice code.</p> <p>Below is an example of repository structure, with separate folder for IaC code (here based on AWS CDK), integration tests under <code>e2</code> folder, <code>src</code> for lambda code, and unit <code>tests</code> folders.</p> <pre><code>autonomous_car_mgr\n\u251c\u2500\u2500 IaC\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 README.md\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 acm\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 __init__.py\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 main_stack.py\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 app.py\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 cdk.json\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 requirements-dev.txt\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 requirements.txt\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 source.bat\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 tests\n\u251c\u2500\u2500 e2e\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 CreateCarRecords.py\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 CreateNewCarViaAPI.py\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 GetCarById.py\n\u251c\u2500\u2500 src\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 carmgr\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 __init__.py\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 acm_model.py\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 app.py\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 requirements.txt\n\u2514\u2500\u2500 tests\n    \u251c\u2500\u2500 requirements-dev.txt\n    \u2514\u2500\u2500 ut\n</code></pre>"},{"location":"dev_ops/#cdk","title":"CDK","text":"<p>CDK uses an imperative programming style which can make complex logic and conditionals easier to implement compared to the declarative style of AWS SAM templates.</p> <p>See an example of CDK application to deploy API Gateway, Lambda function implementing a simple microservice, and DynamoDB table here.. </p> <p>See AWS CDK Python Reference to get examples and API description to create anything in AWS.</p> <p>As a pre-requisite developers need to bootstrap CDK in the target deployment region using <code>cdk bootstrap</code>. </p> <p>CDK code can be validated using</p> <p><pre><code>cdk synth\n</code></pre> which creates a CloudFormation template in cdk.out folder.</p> <p>Then any code or IaC updates can be deployed using:</p> <pre><code>cdk deploy\n</code></pre>"},{"location":"dev_ops/#aws-sam-serverless-application-model","title":"AWS SAM - Serverless Application Model","text":"<p>AWS SAM is an extension of AWS CloudFormation with a simpler syntax for configuring common serverless application resources such as functions, triggers, databases and APIs. It offers the following benefits:</p> <ul> <li>Define the application infrastructure as code quickly, using less code.</li> <li>Manage the serverless applications through their entire development lifecycle.</li> <li>Quickly provision permissions between resources with AWS SAM connectors.</li> <li>SAM CLI provides a Lambda-like execution environment that lets you locally build, test, and debug applications defined by SAM templates or through the AWS Cloud Development Kit (CDK).</li> <li>Continuously sync local changes to the cloud as we develop.</li> <li>On top of CloudFormation or Terraform.</li> </ul> <p>SAM includes two parts:</p> <ol> <li>SAM template specification: It is an extension on top of AWS CloudFormation.</li> <li>A CLI to create new project, build and deploy, perform local debugging and testing, configure pipeline.</li> </ol> <p>During deployment, SAM transforms and expands the SAM syntax into AWS CloudFormation syntax.</p> <p>See SAM templates here</p> <ul> <li>See the Getting started tutorial.</li> <li>A Complete SAM Workshop. and the creating your first API from scratch with OpenAPI and AWS SAM.</li> </ul> <p>SAM CLI supports local development and testing of Lambda apps. To run AWS SAM projects locally, you need to have Docker installed and running on your local machine. it will create a local HTTP server hosting all of template's functions (need to specify the DOCKER_HOST env variable if not, you may get <code>Error: Running AWS SAM projects locally requires Docker. Have you got it installed and running?</code>). </p> <pre><code>DOCKER_HOST=unix:///$HOME/.docker/run/docker.sock sam local start-api\n</code></pre> <p>SAM CLI can also being combined with the template CDK created: basically the CDK defines the lambda functions and other resources, while SAM invoke the function from the definition within the template.</p> <pre><code>cdk synth\n# it will create a cloud formation template under cdk.out folder\nDOCKER_HOST=unix:///$HOME/.docker/run/docker.sock sam local function_name -t  cdk.out/ACSCarMgr.template.json\n</code></pre> <p>Now if the Lambda function has dependencies it is recommended to do a sam build using the template from CDK and then invokes from the created content. The build will look at the requirements.txt for the function and build every assets in the folder <code>.aws-sam/build</code> folder:</p> <pre><code>sam build -t cdk.out/TmpStack.template.json\nDOCKER_HOST=unix:///$HOME/.docker/run/docker.sock sam local invoke MyFunction --no-event\n</code></pre> <p>When testing a request coming from API Gateway we can use an event.json payload </p> <pre><code>DOCKER_HOST=unix:///$HOME/.docker/run/docker.sock sam local invoke MyFunction -e events/getcars.json\n</code></pre> <p>Also CDK provides a full-featured local development environment with watch mode, so updating deployed Lambda function from local update.</p> <pre><code>cdk watch\n</code></pre>"},{"location":"dev_ops/#code-boilerplate","title":"Code boilerplate","text":"<p>AWS Powertools has a REST resolver to annotate function to support different REST resource paths and HTTP verbs. It leads to cleaner code, and easier to test the business logic: each path and HTTP method is implemented by different function:</p> <pre><code>@app.get(\"/cars/&lt;car_id&gt;\")\n@tracer.capture_method\ndef getCarUsingCarId(car_id: str):\n    car = car_table.get_item( Key={'car_id': car_id})\n    logger.debug(car)\n    return car['Item']\n</code></pre> <p>See the app.py code and how to unit test it using pytest test_acr_mgr.py.</p> <p>When using Powertools REST resolver, the API Gateway API needs to be a proxy integration to Lambda as the REST resolver will use the metadata of the request.</p> <pre><code>def handler(message: dict, context: LambdaContext) -&gt; dict:\n    return app.resolve(message, context)\n</code></pre> <p>The Powertools library is integrated as a Lambda layer.</p>"},{"location":"dev_ops/#layers-for-reuse","title":"Layers for reuse","text":"<p>Layers are packages of libraries or other dependencies that can be used by multiple functions.</p> <p>A layer is a ZIP archive that contains libraries, a custom runtime, or other dependencies. See lambda layer management in product documentation.</p> <p>One of the advantage is to see the function code in the Lambda console.</p> <p>When you add a layer to a function, Lambda loads the layer content into the /opt directory of that execution environment. </p> <p>Example of packaging python dependencies as zip, and create a Lambda Layer to be reused. </p> <pre><code>pip3 install --target ./package -r requirements.txt\ncd package\nzip -r ../pck.zip .\n# use CLI for example \naws lambda publish-layer-version --layer-name common-python3 \\\n    --description \"A layer for some python lambda\" \\\n    --license-info \"MIT\" \\\n    --zip-file fileb://pck.zip \\\n    --compatible-runtimes python3.10 python3.11 \\\n    --compatible-architectures \"arm64\" \"x86_64\"\n</code></pre>"},{"location":"dev_ops/#deeper-dive","title":"Deeper dive","text":"<ul> <li>Getting started with Powertools</li> <li>How to use layer to manage common dependencies in Lambda.</li> </ul>"},{"location":"dev_ops/#unit-testing","title":"Unit testing","text":"<p>The unittest can be done using mock library to bypass remote calls. Tests may be defined using pytest library and commands.</p> <p>For example the folder: tests/ut includes test case definitions to test the microservice. </p> <pre><code>pytest -vs\n</code></pre> <ul> <li>Example of test mock for a dynamodb client, using moto library</li> </ul> <pre><code>from moto import mock_aws\n\n@pytest.fixture(scope=\"module\")\ndef dynamodb_client(aws_credentials):\n    \"\"\"DynamoDB mock client.\"\"\"\n    with mock_aws():\n        conn = boto3.client(\"dynamodb\", region_name=\"us-west-2\")\n        yield conn\n</code></pre> <ul> <li>With injection to tune the app under test</li> </ul> <pre><code>@pytest.fixture(scope=\"module\")\ndef getApp(dynamodb_client,populate_table,event_client,secret_client):\n</code></pre> <p>See the full code in test_acr_mgr.py.</p>"},{"location":"dev_ops/#deployment","title":"Deployment","text":"<p>Package the code as ZIP file (250MB limit) or container image (with 10GB limit).</p> <p>Lambda can use Layer to simplify code management and reuse. The Powertools library for example can be used as a layer.</p> <pre><code>powertools_layer = aws_lambda.LayerVersion.from_layer_version_arn(\n            self,\n            id=\"lambda-powertools\",\n            layer_version_arn=f\"arn:aws:lambda:{env.region}:017000801446:layer:AWSLambdaPowertoolsPythonV2:61\"\n        )\n\n# referenced in the function definition\n acm_lambda = aws_lambda.Function(self, 'CarMgrService',\n     ...\n    layers=[powertools_layer],\n</code></pre> <p>But local dependencies can also being deployed as a zip (See the product documentation- Working with .zip file archives for Python) for the explanations on how to do it. With CDK it is possible to use the concept of bundling options:</p> <pre><code> code= aws_lambda.Code.from_asset(path=\"../src/\",\n        bundling=BundlingOptions(\n            image= aws_lambda.Runtime.PYTHON_3_11.bundling_image,\n            command= [\n                'bash','-c','pip install -r requirements.txt -t /asset-output &amp;&amp; cp -rau . /asset-output'\n            ],\n    )),\n</code></pre> <p>In case of import error on Lambda execution, ask Amazon Q and re:Post, some library bundled on Mac will not work on other Lambda runtime.</p>"},{"location":"dev_ops/#integration-tests","title":"Integration tests","text":"<p>Once the solution is deployed we can test by using the API Gateway API endpoints. The tests can be defined using Python script files (under e2e folder).</p> <pre><code>python GetCarByIdUsingAPI.py 2\n</code></pre>"},{"location":"dev_ops/#error-handling-by-execution-model","title":"Error Handling by execution model","text":"<p>As a general practice, implement error handling within each function rather than propagating errors up the call chain. This prevents failures in one function from impacting others.</p> <p>Implement throttling, retries and DLQs between functions to avoid failures in one function causing cascading failures in others.</p> <p>There are different ways to support error handling, depending of the integration and communication protocol.</p> API Gateway - synchronous call <ul> <li>Timeout \u2013 API Gateway has a 30-second timeout. If the Lambda function hasn't responded to the request within 30 seconds, an error is returned to the client caller.</li> <li>Retries \u2013 There are no built-in retries if a function fails to execute.</li> <li>Error handling \u2013 Generate the SDK from the API stage, and use the backoff and retry mechanisms it provides.</li> </ul> SNS - asynch <ul> <li>Timeout \u2013 Asynchronous event sources do not wait for a response from the function's execution. Requests are handed off to Lambda, where they are queued and processed by Lambda.</li> <li>Retries \u2013 Asynchronous event sources have built-in retries. If a failure is returned from a function's execution, Lambda Service will attempt that invocation two more times for a total of three attempts to execute the function with its event payload. We can use the Retry Attempts configuration to set the retries to 0 or 1 instead. If Lambda is unable to invoke the function (for example, if there is not enough concurrency available and requests are getting throttled), Lambda will continue to try to run the function again for up to 6 hours by default. We can modify this duration with Maximum Event Age.</li> <li>Error handling - Use the Lambda destinations OnFailure option to send failures to another destination for processing. Alternatively, move failed messages to a dead-letter queue on the function.</li> </ul> Kinesis Data Streams <ul> <li>Timeout \u2013 When the retention period for a record expires, the record is no longer available to any consumer (24h). As an event source for Lambda, we can configure <code>Maximum Record Age</code> to tell Lambda to skip processing a data record.</li> <li>Retries \u2013  By default, Lambda retries a failing batch until the retention period for a record expires. We can configure <code>Maximum Retry Attempts</code> so that the Lambda function will skip retrying a batch of records.</li> <li>Error handling - Configure an OnFailure destination on the Lambda function so that when a data record reaches the <code>Maximum Retry Attempts</code> or <code>Maximum Record Age</code>, we can send its metadata, such as shard ID and stream Amazon Resource Name (ARN), to an SQS queue or SNS topic.</li> </ul> SQS Queue <ul> <li>Timeout \u2013 When the visibility timeout expires, messages become visible to other consumers on the queue. Set our visibility timeout to 6 times the timeout we configure for our function.</li> <li>Retries \u2013 Use the <code>maxReceiveCount</code> on the queue's policy to limit the number of times Lambda will retry to process a failed execution.</li> <li>Error handling - Write our functions to delete each message as it is successfully processed. Move failed messages to a dead-letter queue configured on the source SQS queue.</li> </ul> Dead-letter <p>We may turn on dead-letter queues and create dedicated dead-letter queue resources using Amazon SNS or Amazon SQS for individual Lambda functions that are invoked by asynchronous event source.</p>"},{"location":"dev_ops/#monitoring","title":"Monitoring","text":""},{"location":"dev_ops/#cloudwatch-monitoring","title":"CloudWatch monitoring","text":"<p>Lambda monitoring is supported by Amazon CloudWatch, with logs and metrics such as invocation count, duration, and errors. Within CloudWatch, DevOps team may set alarms on certain metrics that may breach, to create notifications to the team.</p> <p>Logs need to be added to the Function code. (print() in Python goes to the log). Use boilerplate code like AWS Powertools to control the logging level, participate in Xray reporting to view detailed traces of requests across your application and Lambda functions. This helps pinpoint where latency is occurring.</p>"},{"location":"dev_ops/#using-aws-powertools","title":"Using AWS Powertools","text":"<p>AWS Powertools includes methods and libraries for quickly implementing consistent logging, metrics, and traces inside your codebase.</p> <p>Use X-Ray to understand end to end event flow, cross components of your solution.</p>"},{"location":"dev_ops/#asynchronous-processing","title":"Asynchronous processing","text":"<p>While messages go to dead letter queues to handle errors and monitor messages sent to the DLQ. This helps track failed invocations.</p>"},{"location":"dev_ops/#cicd-pipeline","title":"CI/CD pipeline","text":"<p>We recommend to follow this workshop - Building CI/CD pipelines for Lambda canary deployments using AWS CDK as a good source on how to do blue/green deployment.</p> <p>The autonomous car manager service use the concepts of this workshop, like alias, function versioning, CodeDeploy application.</p>"},{"location":"dev_ops/#version-management-rollback","title":"Version management - Rollback","text":"<p>Lambda supports versioning and developer can maintain one or more versions of the lambda function. We can reduce the risk of deploying a new version by configuring the <code>alias</code> to send most of the traffic to the existing version, and only a small percentage of traffic to the new version. Below  is an example of creating one Alias to version 1 and a routing config with Weight at 30% to version 2. Alias enables promoting new lambda function version to production and if we need to rollback a function, we can simply update the alias to point to the desired version. Event source needs to use Alias ARN for invoking the lambda function.</p> <pre><code>aws lambda create-alias --name routing-alias --function-name my-function --function-version 1  --routing-config AdditionalVersionWeights={\"2\"=0.03}\n</code></pre>"},{"location":"dev_ops/#bluegreen","title":"Blue/Green","text":""},{"location":"dev_ops/#canary-deployment","title":"Canary deployment","text":"<p>Canary deployment works similarly to blue-green deployment but use a small set of servers before finishing the others. With Lambda to achieve canary deployments we need to use AWS CodeDeploy.</p> <p>CodeDeploy application is the Lambda version. To enable traffic shifting deployments for Lambda functions, CodeDeploy uses Lambda Aliases, which can balance incoming traffic between two different versions of your function. When you publish a new version of the function to your stack, CodeDeploy will send a small percentage of traffic to the new version, monitor, and validate before shifting 100% of traffic to the new version.</p> <p>Below is an example of creating a CodeDeploy application and deployment group to use the last created alias version:</p> <pre><code>def code_deploy_app(self,fctAlias):\n        application=  aws_codedeploy.LambdaApplication(self, \"AcmCodeDeploy\",\n            application_name=\"AcmMicroservice\"\n        )\n        deployment_group = aws_codedeploy.LambdaDeploymentGroup(self, \"AcmBlueGreenDeployment\",\n            application=application,  # optional property: one will be created for you if not provided\n            alias=fctAlias,\n            deployment_config=aws_codedeploy.LambdaDeploymentConfig.LINEAR_10_PERCENT_EVERY_1_MINUTE\n        )\n        return application\n</code></pre> <p>When the CloudFormation has created the CodeDeploy application, we can see the deployment of the Lambda occurring</p> <p></p> <p>And Lambda having 2 versions with different traffic ratio:</p> <p></p>"},{"location":"dev_ops/#other-tools","title":"Other tools","text":""},{"location":"dev_ops/#cloud9","title":"Cloud9","text":"<p>Cloud9 IDE is a nice solution to get a vscode compatible IDE on linux, so Lambda code can be compatible with runtime.</p> <p>Some quick installation updates</p> <ul> <li>update python, under environment</li> </ul> <pre><code>python3 --version\n&gt;&gt; \nwget https://www.python.org/ftp/python/3.11.8/Python-3.11.8.tgz               \ntar xzf Python-3.11.8.tgz \n\nsudo ./configure --enable-optimizations \nsudo make altinstall \n# The Python binary will be available under the /usr/local/bin directory.\npython3.11 --version\n</code></pre> <p>To work on the autonomous-car-mgr, git clone the repository. </p> <p>Then for:</p> <ul> <li> <p>Start a Python virtual env</p> <pre><code>python3.11 -m venv .devenv\nsource .devenv/bin/activate\n</code></pre> </li> <li> <p>Package with the Python libraries dependencies for Linux</p> <pre><code># Under src\npip install  -r requirements.txt -t package\ncd package\nzip -r ../package.zip .\ncd .. &amp;&amp; rm -r package\n</code></pre> </li> <li> <p>CDK deployment, be sure to install cdk in a Python 11 virtual env:</p> <pre><code>python3.11 -m venv .devenv\nsource .devenv/bin/activate\n# under cdk\npip install -r requirements.txt\n# Verify \ncdk lynch\n# Do a stack update\ncdk deploy\n</code></pre> </li> <li></li> </ul>"},{"location":"scaling/","title":"Scaling","text":"<p>When looking at application scaling it is important to do end to end performance testing, and review each capabilities and service limits of the services the application integrate with. Considering assessing timeouts, retry behaviors, throughput, and payload size.</p>"},{"location":"scaling/#performance-considerations","title":"Performance considerations","text":"<p>Lambda invokes the code in a secure and isolated execution environment, which needs to be initialized and then executes the function code for the unique request it handles. </p> <p></p> <p>Second request will not have the initialization step. When requests arrive, Lambda reuses available execution environments, and creates new ones if necessary. The number of execution environments determines the concurrency. Limited to 1000 by default. This is a soft limit that could be increased.</p> <p>Concurrency (# of in-flight requests the function is currently handling) is subject to quotas at the AWS Account and Region level.</p> <p>With API Gateway, there are configuration options for each API, that may help to improve access: like with using Lambda Edge-optimized endpoint, using CloudFront distribution, or use distributed cache to avoid reaching backend.</p> <p>Here are some good questions to address during design time:</p> <ul> <li>Who will be using each API?</li> <li>Where will they be accessing it from, and how frequently?</li> <li>What are the downstream systems that could impact performance?</li> <li>Do you need to throttle requests at the \"front door\" to prevent overloading a downstream component?</li> </ul> <p>When the number of requests decreases, Lambda stops unused execution environments to free up scaling capacity for other functions.</p> <p>Use the Lambda CloudWatch metric named <code>ConcurrentExecutions</code> to view concurrent invocations for all or individual functions.</p> <p>To estimate concurrent requests use:  Request per second x Avg duration in seconds = concurrent requests.</p> <p>The diagram below illustrates the major concepts to understand this concurrency limit: At time T1, the lambda processes 3 requests and performs the init phase for 3 execution environments (it could goes to 1000). After the init, new requests can be processed, but new requests on top of the 3, will create new runtime environment with an init phase. At T3, we are on steady state at 5 concurrent requests. Each request taking 30s, we can see that, once reaching the (burst) quotas, requests can wait 30s to get processed. </p> <p></p> <p>Lambda scales to very high limits, but not all account's concurrency quota is available immediately, so requests could be throttled for a few minutes in case of burst. The concurrency limit is cross Lambda functions within one account and one region.</p> <p>Obviously most of the lambda processing is done under a second, event few milliseconds, so the scaling numbers are higher that the diagram above. </p> <p>There are two scaling quotas to consider with concurrency. Account concurrency quota (1000 per region) and burst concurrency quota (from 500 to 3000 per min per region). Further requests are throttled, and lambda returns HTTP 429 (too many requests).</p> <p>It is possible to use Reserved concurrency, which splits the pool of available concurrency into subsets. A function with reserved concurrency only uses concurrency from its dedicated pool. This is helpful to avoid one lambda function to take all the concurrency quota and impact other functions in the same region. No extra charges.</p> <p>For functions that take a long time to initialize, or that require extremely low latency for all invocations, provisioned concurrency enables to pre-initialize instances of the function and keep them running at all times.</p> <p>Use <code>concurrency limit</code> to guarantee concurrency availability for a function, or to avoid overwhelming a downstream resource that the function is interacting with.</p> <p>Monitor metrics like invocation duration and throttle count to identify any bottlenecks.</p> <p>If the test results uncover situations where functions from different applications or different environments are competing with each other for concurrency, developers probably need to rethink the AWS account segregation strategy and consider moving to a multi-account strategy.</p>"},{"location":"scaling/#performance-optimization","title":"Performance optimization","text":"<p>Some recommendations:</p> <ul> <li>CPU capacity is allocated proportionally to the amount of allocated memory.</li> </ul> <p>While zooming into the cold start phase,</p> <p></p> <ul> <li>We can see that reducing code size and reduce=ing dependencies to the necessary by removing unnecessary files, compressing assets, and optimizing dependencies will help reduce time to start. Smaller packages initialize faster. To better packaging use tools like Maven, Gradle, WebPack, esbuild.  </li> <li>Use code outside of the function handler to initialize connections to back end.</li> <li>Only import what is necessary for your function to run. Each additional import adds to initialization time.</li> <li>Reduce the time to create new environment by using provisioned concurrency: </li> <li>Use AWS Powertools to add observability to get measures and identify cold start. It can reports to AWS X-Ray.</li> <li>Consider worse and best cold start timings to assess where are current limits. Same for worse and best warm start.</li> <li>When integrating with RDS, try to adopt RDS proxy to cache read queries, but also brings connection pooling, keeping connection secrets.</li> <li>Move expensive startup tasks like loading large datasets or models to a separate \"initialization\" function. Cache the results and reuse them across invocations instead of reloading on each cold start.</li> <li>ElasticCache for Redis, can be used to keep some of the read/only response to avoid reaching a slow backend each time a client application is asking those kind of stable data. One of the benefit is that Caches are shared between runtime environment.</li> <li>Consider also closer to client application, caching approaches using API Gateway cache or CloudFront.</li> <li>Move to Graviton hardware, if code supports ARM architecture.</li> <li>Consider using Python's multiprocessing library to parallelize startup tasks like model loading to improve cold start performance.</li> <li>When accessing parameter store from AWS Systems Manager (Using Parameter Store parameters in AWS Lambda functions).</li> <li>Consider using provisioned concurrency for critical functions to avoid cold starts altogether for a pool of pre-initialized functions.</li> </ul>"},{"location":"scaling/#memory","title":"Memory","text":"<p>Memory is the only setting that can impact performance. Both CPU and I/O scale linearly with memory configuration. CPU-bound functions often see improved performance and reduced duration by allocating more memory. We can allocate up to 10 GB of memory to a Lambda function. In case of low performance, start by adding memory to the lambda.</p> <p>Consider using, Lambda Power Tuning,  an open source tool that tests a Lambda function with different memory size, returning the average response time for every memory size tested (it includes cold and warm starts).</p>"},{"location":"scaling/#different-hardware","title":"Different Hardware","text":"<p>The move from x86 to Graviton 2 (ARM architecture) may improve performance at a lower cost.</p>"},{"location":"scaling/#sqs-and-lambda","title":"SQS and Lambda","text":"<p>When Amazon SQS is used as a Lambda event source, the Lambda service manages polling the queue. We can set the batch size, concurrency limit, and timeout. Lambda defaults to using five parallel processes to get messages off a queue. If the Lambda service detects an increase in queue size, it will automatically increase how many batches it gets from the queue, each time. The maximum concurrency is 1000.</p> <p>SQS will continue to try a failed message up to the maximum receive count specified in the re-drive policy, at which point, if a dead-letter queue is configured, the failed message will be put into the dead-letter queue and deleted from your SQS queue.</p> <p>If the visibility timeout expires, before your Lambda function has processed the messages in that batch, any message in that batch that hasn\u2019t been deleted by your function will become visible again. This may generate duplicate processing on the same message. The best practice, is to set your visibility timeout to six times the function timeout.</p>"},{"location":"scaling/#load-testing","title":"Load testing","text":"<p>Perform load testing, close to the business process flow within the different component, to assess which quotas may need some tuning.</p> <p>The potential tools to support load testing are:</p> <ul> <li>Apache JMeter</li> <li>Artillery Community Edition</li> <li>Gatling</li> </ul>"},{"location":"scaling/#interesting-source-of-information","title":"Interesting source of information","text":"<ul> <li>AWS Lambda Power Tuning tool to find the right memory configuration.</li> <li>Understanding AWS Lambda scaling and throughput - an AWS blog.</li> <li>How to improve Lambda Warm Start Speed by 95%!</li> <li>Interesting github for lambda optimization.</li> <li>Lambda Power Tuning.</li> <li>How do memory and computing power affect AWS Lambda cost?</li> <li>AWS re:Invent 2022 - A closer look at AWS Lambda (SVS404-R)</li> <li>Load testing with Artillery</li> <li>Serverless Snippets Collection - CloudWatch Logs Insights.</li> <li>Blog - We Improved Our Lambda Warm Start Speed by 95%! Here\u2019s How. with git repository</li> </ul>"},{"location":"security/","title":"Security in Lambda context","text":"<p>As a managed service, AWS manages the underlying infrastructure and foundation services, the operating system, and the application platform. Developers need to ensure code, libraries, configuration, IAM are well set.</p>"},{"location":"security/#shared-responsibility","title":"Shared responsibility","text":"<p>With the AWS Cloud, managing security and compliance is a shared responsibility between AWS and the customer:</p> <ul> <li>AWS is responsible of security of the cloud and offers the most flexible and secure cloud computing environment available today. AWS is responsible for patching their managed services and infrastructure security.</li> <li>Customers are responsible for the security in the cloud: secure workloads and applications that are deployed onto the cloud. When using EC2, we are responsible to patch OS for security (but AWS helps by providing patched AMIs, or tools such as Systems Manager, or Inspector for continuous vulnerability testing).</li> </ul>"},{"location":"security/#the-different-security-actors-in-lambda-context","title":"The different security actors in Lambda context","text":"<ol> <li> <p>A developer who need to access Lambda Management Console, deploy code, review logs. The AWS account IAM administrator needs to authorize access via user group and IAM policies.</p> </li> <li> <p>Authenticated \"user\" includes the AWS account root user, an IAM user (developer), or an assumed IAM role. It is possible to use federated identity to authenticate users.</p> </li> <li> <p>The client applications calling the Lambda function: the control is done via resource policies. Resource policies are created when we add a trigger to a Lambda function, or defined when specifying a \"push\" event source such as Amazon API Gateway. Policy allows the event source to take the <code>lambda:InvokeFunction</code> action. Restrict access to only required principals.</p> </li> <li>The code within the lambda function to act upon other services is controlled via IAM execution role. This role  must include a trust policy that allows Lambda to \u201cAssumeRole\u201d so that it can take that action for another service.</li> </ol>"},{"location":"security/#policies-best-practices","title":"Policies best practices","text":"<p>See product documentation and IAM Access Analyzer tool to validate your own policy.</p> <p>For execution role consider the AWS managed policies such as <code>AWSLambdaBasicExecutionRole</code>, <code>CloudWatchLambdaInsightsExecutionRolePolicy</code> and <code>AWSXRayDaemonWriteAccess</code>.</p> <p>Use IAM Access Analyzer to generate least-privilege policies based on access activity.</p>"},{"location":"security/#example-of-ensuring-least-privilege","title":"Example of ensuring least privilege","text":"<p>The IAM user guide, Apply least-privilege permissions  section includes the best practices to get started and reduce the permissions. Start by using the AWS managed policies that grant permissions for many common use cases and then define your own customer managed policies.</p> <p>Implement fine-grained access control using IAM policies. For example, use conditions to restrict which functions can invoke other functions.</p> <p>The CDK code for Lambda function to access DynamoDB table is</p> <pre><code>carTable.grant_read_write_data(acm_lambda)\n</code></pre> <p>And generates the following policy:</p> <pre><code>\"Statement\": [\n    {\n        \"Action\": [\n            \"dynamodb:BatchGetItem\",\n            \"dynamodb:BatchWriteItem\",\n            \"dynamodb:ConditionCheckItem\",\n            \"dynamodb:DeleteItem\",\n            \"dynamodb:DescribeTable\",\n            \"dynamodb:GetItem\",\n            \"dynamodb:GetRecords\",\n            \"dynamodb:GetShardIterator\",\n            \"dynamodb:PutItem\",\n            \"dynamodb:Query\",\n            \"dynamodb:Scan\",\n            \"dynamodb:UpdateItem\"\n        ],\n        \"Resource\": \"arn:aws:dynamodb:us-west-2:927698749254:table/acm_cars\",\n        \"Effect\": \"Allow\"\n    }],\n</code></pre> <p>See other examples from product documentation.</p> <p>If developer needs to be more restrictive, it is possible to define custom policy and attach to a role used by the Lambda function. CDK, CloudFormation or SAM offer such constructs.</p> <pre><code>sm_role.add_managed_policy(iam.ManagedPolicy.from_aws_managed_policy_name(\"service-role/AWSLambdaBasicExecutionRole\"))\nsm_role.add_managed_policy(iam.ManagedPolicy.from_aws_managed_policy_name(\"AWSXRayDaemonWriteAccess\"))\nsm_role.add_managed_policy(iam.ManagedPolicy.from_aws_managed_policy_name(\"service-role/AWSLambdaVPCAccessExecutionRole\"))\n</code></pre>"},{"location":"security/#secops-considerations","title":"SecOps considerations","text":"<p>As a DevOps consider the followings:</p> <ul> <li>the Data privacy with encryption at rest with customer managed Key.</li> <li>Lambda only supports secure connections over HTTPS (TLS1.2)</li> <li>Environment variables are secured by encryption using a Lambda-managed KMS key (named <code>aws/lambda</code>). The <code>CreateFunction</code> API or <code>UpdateFunctionConfiguration</code> may use KMS Keys.</li> <li>Do not store sensitive information inside a Lambda function.</li> <li>Monitor and log function execution using services like AWS CloudWatch Logs, X-Ray, and GuardDuty. AWS X-Ray also encrypts data by default.</li> <li>Run on EC2 with Nitro System for better security isolation.</li> <li>Monitor access and permissions using services like AWS CloudTrail and AWS Config. These provide visibility into who/what is invoking your functions.</li> <li>Code releases go through security review and penetration testing.</li> <li>Lambda provides a code signing feature to ensure only trusted code runs in the Lambda function.</li> <li>Use AWS WAF with API Gateway or CloudFront to filter requests to your functions based on IP addresses or request attributes.</li> <li>Each Lambda exec environment includes a writeable /tmp folder: files written within it, remain for the lifetime of the execution environment.</li> </ul>"},{"location":"security/#environment-variables","title":"Environment Variables","text":"<p>Environment Variables in Lambda helps to avoid putting credentials, endpoints URL, etc, in code. Infrastructure as code can help linking created resource, like a dynamoDB table with a lambda function.</p> <pre><code>acm_lambda = aws_lambda.Function(self, 'CarMgrService',\n            runtime=aws_lambda.Runtime.PYTHON_3_11,\n            function_name= \"CarMgrService\",\n            environment = {\n                \"CAR_EVENT_BUS\":carEventBus.event_bus_name,\n                \"CAR_TABLE_NAME\":carTable.table_name,\n                \"POWERTOOLS_SERVICE_NAME\": \"CarManager\",\n                \"POWERTOOLS_METRICS_NAMESPACE\": \"CarManager\",\n                \"POWERTOOLS_LOG_LEVEL\": \"INFO\",\n            },\n</code></pre> <p>When you publish a version, the environment variables are locked for that version.</p> <p>Lambda runtimes set several environment variables (AWS_REGION, AWS_EXECUTION_ENV...) during initialization. Most of the environment variables provide information about the function or runtime.</p> <p>Environment variables can be encrypted using server-side encryption with an AWS KMS key. When using default key there is no need to get access permission, while when using customer managed key, Lambda needs grant access to on the key</p>"},{"location":"security/#secrets-in-aws-secrets","title":"Secrets in AWS Secrets","text":"<p>Lambda can access AWS Secrets and then uses the secret to connect to a service, a private resource or an API.</p> <ol> <li> <p>Create a secret using AWS CLI</p> <pre><code>DB_PASSWORD=$(dd if=/dev/random bs=8 count=1 2&gt;/dev/null | od -An -tx1 | tr -d ' \\t\\n')\naws secretsmanager create-secret --name rds-postgres-admin --description \"database password\" --secret-string \"{\\\"username\\\":\\\"postgres\\\",\\\"password\\\":\\\"$DB_PASSWORD\\\"}\"\n</code></pre> </li> <li> <p>Or via CDK</p> <pre><code>secrets=aws_secretsmanager.Secret(self,\n                              \"ACMsecrets\",\n                            secret_name=DEFAULT_SECRET_NAME,\n                            generate_secret_string=aws_secretsmanager.SecretStringGenerator(\n                                secret_string_template=json.dumps({\"username\": \"postgres\"}),\n                                generate_string_key=\"password\",\n                                exclude_characters=\"/@\"\n                            ))\nsecrets.grant_read(grantee=acm_lambda)\n</code></pre> </li> <li> <p>Pass the secret name as environment variable to the Lambda function</p> </li> <li> <p>In function code, access Secret Manager using SDK</p> <pre><code>secret_name=os.getenv(\"secret_name\")\nsecret_client=boto3.client(\"secretsmanager\")\nsecret_value_response = secret_client.get_secret_value(\n        SecretId=secret_name\n    )\nusername=secret_value_response['username']\n</code></pre> </li> </ol>"},{"location":"security/#access-to-vpc-resources","title":"Access to VPC resources","text":"<p>Lambda function can access resources inside a private VPC. Developer needs to provide additional VPC-specific configuration information that includes VPC subnet IDs and security group IDs and the <code>AWSLambdaVPCAccessExecutionRole</code> policy. AWS Lambda uses this information to set up elastic network interfaces (ENIs), for each subnet, that enable the function to connect securely to other resources in the VPC.</p> <p></p> <p>ENI represents a virtual network card</p> <p>As Lambda function always runs inside a VPC owned by the Lambda service, the function accesses resources in our VPC using a Hyperplane ENI. Hyperplane ENIs provide NAT capabilities from the Lambda VPC to our account VPC using VPC-to-VPC NAT (V2N).</p> <p>If it runs out of IP@ then we will have EC2 error types like <code>EC2ThrottledException</code> and the function will not scale. So be sure to have multiple AZ/ subnets and enough IP@.</p> <p>For the function to access the internet, route outbound traffic to a NAT gateway in one public subnet in our VPC.</p> <p>To establish a private connection between the VPC and Lambda, create an interface VPC endpoint (using AWS PrivateLink). Traffic between the VPC and Lambda does not leave the AWS network.</p> <pre><code>aws ec2 create-vpc-endpoint --vpc-id vpc-ec43eb89 --vpc-endpoint-type Interface --service-name \\\ncom.amazonaws.us-west-2.lambda --subnet-id subnet-abababab --security-group-id sg-1a2b3c4d      \n</code></pre> <p>Endpoint policy can be attached to the interface endpoint to add more control on which resource inside the VPC can access the lambda function (Principal = user, Allow lambda:InvokeFunction on resource with the function arn).</p>"},{"location":"websocket-lambda/","title":"Websocket connection with client app and Lambda backend","text":"<p>The client can be a mobile app with a push mechanism to get data from backend feed to the app, or a chat bot to maintain a connection with the bot backend and the client app.</p>"},{"location":"websocket-lambda/#architecture","title":"Architecture","text":"<p>The architecture uses a set of Lambda and API Gateway with SebScoket protocol. The SAM extract looks like</p> <pre><code>Websocket:\n    Type: AWS::ApiGatewayV2::Api\n    Properties:\n      Name: SessionManagementWebsocketApi\n      ProtocolType: WEBSOCKET\n      RouteSelectionExpression: \"$request.body.action\"\n</code></pre> <p>The client app, creates a unique id, and establishes a connection to the API gateway with <code>$connect</code> which routes to a Lambda, as declared in this SAM section:</p> <pre><code>ConnectRoute:\n    Type: AWS::ApiGatewayV2::Route\n    Properties:\n      ApiId: !Ref Websocket\n      RouteKey: $connect\n      AuthorizationType: NONE\n      OperationName: ConnectRoute\n      Target: !Join [ '/', ['integrations', !Ref ConnectInteg] ]\n  ConnectInteg:\n    Type: AWS::ApiGatewayV2::Integration\n    Properties:\n      ApiId: !Ref Websocket\n      Description: Connect Integration\n      IntegrationType: AWS_PROXY\n      IntegrationUri: !Sub arn:aws:apigateway:${AWS::Region}:lambda:path/2015-03-31/functions/${OnConnectFunction.Arn}/invocations\n</code></pre> <p>The unique ID is kept in the session browser data store. </p> <p></p> <p>The unique ID is added to the Sec-WebSocket-Protocol header. Another way to pass the unique ID to the backend could be a query string parameter. Keep the Sec-WebSocket-Protocol in the backend response:</p> <pre><code>{\n    'statusCode': 200,\n    'headers': {\n        'Sec-WebSocket-Protocol': userId\n    }\n}\n</code></pre> <p>If using authentication with Oauth service, the ID  comes from a JWT token or other form of authentication and uses it instead of a randomly generated unique ID.</p> <p>In API Gateway, the WebSocket APIs are routes integrated with HTTP backend, Lambda or other supporting services. The connection is bi-directional. The WebSocket API generates the connection ID automatically.</p> <p>Client app sends a WebSocket upgrade request. If the request succeeds, the <code>$connect</code> route is executed while the connection is being established. Authorization should be done in the $connect. Use $connect when we need to be aware of the client app, and be able to send messages from backend. When we need to maintain connection information with the different connected client apps, then a connection ID is used and an external database need to keep information of the connection ID, user id, and may the state of the communication. The primary key is the user id, this helps locate users faster as they reconnect.</p> <pre><code>ddb.put_item(\n        TableName=table_name,\n        Item={\n            'userId': {'S': userId},\n            'connectionId': {'S': connection_id},\n            'domainName': {'S': domain_name},\n            'stage': {'S': stage},\n            'lastSeen' : {'N': last_seen},\n            'active': {'S': True}\n        }\n    )\n</code></pre> <p>Using the <code>put_item</code> function, we don\u2019t need to query the DB if the user already exists. If it is a new user, Put creates a new item. Use a global secondary index in DynamoDB to locate the connection ID, to mark the connection inactive when WebSocket API calls <code>OnDisconnect</code>.</p> <p>See the SAM template declaration for the DynamoDB tables, keys and global indexes</p> <pre><code>ConnectionsTable:\n    Type: AWS::DynamoDB::Table\n    Properties:\n      AttributeDefinitions:\n      - AttributeName: \"userId\"\n        AttributeType: \"S\"\n      - AttributeName: \"connectionId\"\n        AttributeType: \"S\"\n      - AttributeName: \"active\"\n        AttributeType: \"S\"\n      - AttributeName: \"lastSeen\"\n        AttributeType: \"N\"\n      KeySchema:\n      - AttributeName: \"userId\"\n        KeyType: \"HASH\"\n      GlobalSecondaryIndexes:\n      - IndexName: connectionId-index\n        Projection:\n          ProjectionType: ALL\n        ProvisionedThroughput:\n          WriteCapacityUnits: 5\n          ReadCapacityUnits: 5\n        KeySchema:\n        - KeyType: HASH\n          AttributeName: connectionId\n      - IndexName: lastSeen-index\n        Projection:\n          ProjectionType: KEYS_ONLY\n        ProvisionedThroughput:\n          WriteCapacityUnits: 5\n          ReadCapacityUnits: 5\n        KeySchema:\n        - KeyType: HASH\n          AttributeName: active\n        - KeyType: RANGE\n          AttributeName: lastSeen\n      ProvisionedThroughput:\n        ReadCapacityUnits: 5\n        WriteCapacityUnits: 5\n      SSESpecification:\n        SSEEnabled: True\n\n  SessionsTable:\n    Type: AWS::DynamoDB::Table\n    Properties:\n      AttributeDefinitions:\n      - AttributeName: \"userId\"\n        AttributeType: \"S\"\n      KeySchema:\n      - AttributeName: \"userId\"\n        KeyType: \"HASH\"\n      ProvisionedThroughput:\n        ReadCapacityUnits: 5\n        WriteCapacityUnits: 5\n      SSESpecification:\n        SSEEnabled: True\n</code></pre> <p>Once the record in a table, the application has connection management, and can retrieve session data. The business function, is specific to each business application, but will need to search for the connection ID, using the DynamoDB index and get the state of the connection. </p> <pre><code>def get_user_id(connection_id):\n    response = ddb.query(\n        TableName=connections_table_name,\n        IndexName='connectionId-index',\n        KeyConditionExpression='connectionId = :c',\n        ExpressionAttributeValues={\n            ':c': {'S': connection_id}\n        }\n    )\n\n    items = response['Items']\n\n    if len(items) == 1:\n        return items[0]['userId']['S']\n\n    return None\n</code></pre> <p>Another table could be added to maintain the state of the session. For example if the source of the messages to send back to a client comes from Kafka, it can be the last commited offset, or if messages are coming from a FIFO SQS Queue, it can be a time stamp of the last pushed message, or the last message itself. The code to push to the websocket looks like this:</p> <pre><code>try:\n    api_client.post_to_connection(\n        ConnectionId=connection_id,\n        Data=bytes(message_to_send, 'utf-8')\n    )\nexcept api_client.exceptions.GoneException as e:\n    print(f\"Found stale connection, persisting state\")\n    store_cursor_position(user_id, position_in_data_source)\n    return {\n        'statusCode': 410\n    }\n</code></pre> <p>In chat bot a snapshot of the conversation needs to be set up and persisted. </p> <p>The <code>$disconnect</code> route is executed after the connection is closed. The active attribute in the connection table is set to false. Session record may be kept. </p> <p>The delete function is run every x minutes and look at potential non-active connection or connection too old (user did not reconnect). Or use the onDelete WebSocket verb to route to a lambda doing the cleaning in connection and session tables based on the unique ID.</p> <p>When using DynamoDB, it provides a built-in mechanism for expiring items called Time to Live (TTL)</p>"},{"location":"websocket-lambda/#sources","title":"Sources","text":"<ul> <li>websocket-sessions-management sample repository.</li> </ul>"}]}